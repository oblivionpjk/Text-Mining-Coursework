# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(8,0.3))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(8,0.2))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(8,0.1))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(8,0.5))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 10,  scale=c(8,0.5))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 20,  scale=c(8,0.5))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=80, col=rainbow(50),min.freq = 5,  scale=c(8,0.5))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=50, col=rainbow(50),min.freq = 5,  scale=c(8,0.5))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(4,0.5))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(4,0.1))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(4,0.8))
# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(4,0.6))
# for knitting the document and enabling the include_graphics function
library(knitr)
library(png)
# for determining the dimensions of PNG files
include_graphics("wordcloud_tw.png")
#install.packages("LDAvis")
#install.packages("tm")
#install.packages("lda")
#install.packages("servr")
#install.packages("shiny")
#install.packages("stringr")
library(LDAvis)
library(tm)
library(lda)
library(shiny)
library(stringr)
stop_words <- stopwords("SMART")
Dataset2<-read.csv("witcher3_tw_cn.csv")
tweet <- Dataset2$x
tweet <- sapply(tweet, function(x) iconv(x, to='UTF-8', sub='byte'))
tweet= gsub("[[:punct:]]", "", tweet)
tweet = gsub("[[:digit:]]", "", tweet)
tweet= gsub("http\\w+", "", tweet)
tweet = gsub("[ \t]{2,}", "", tweet)
tweet= gsub("^\\s+|\\s+$", "", tweet)
#ref: ( Hicks , 2014)
#get rid of unnecessary spaces
tweet <- str_replace_all(tweet," "," ")
tweet <- str_replace(tweet,"RT @[a-z,A-Z]*: ","")
# Get rid of hashtags
tweet <- str_replace_all(tweet,"#[a-z,A-Z]*","")
# Get rid of references to other screennames
tweet<- str_replace_all(tweet,"@[a-z,A-Z]*","")
# tokenize on space and output as a list:
doc.list <- strsplit(tweet, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents
W <- length(vocab)  # number of terms in the vocab
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document
N <- sum(doc.length)  # total number of tokens in the data
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus
# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = 10, vocab = vocab,
num.iterations = 200, alpha = 0.5, eta=0.5,
initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1
#LDAvis
theta <- t(apply(fit$document_sums + 0.5, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + 0.5, 2, function(x) x/sum(x)))
tweetvis <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
# create the JSON object to feed the visualization:
json <- createJSON(phi = tweetvis$phi,
theta = tweetvis$theta,
doc.length = tweetvis$doc.length,
vocab = tweetvis$vocab,
term.frequency = tweetvis$term.frequency)
serVis(json, out.dir = tempfile(), open.browser = interactive())
library(knitr)    # For knitting document and include_graphics function
library(png)
# For grabbing the dimensions of png files
include_graphics("topic_tw.png")
library(knitr)    # For knitting document and include_graphics function
library(png)
# For grabbing the dimensions of png files
include_graphics("topic_tw.png")
# for knitting the document and enabling the include_graphics function
library(knitr)
library(png)
# for determining the dimensions of PNG files
include_graphics("wordcloud_tw.png")
library(knitr)    # For knitting document and include_graphics function
library(png)
# For grabbing the dimensions of png files
include_graphics("map_tw.png")
#\captionof{figure}{caption}
library(knitr)    # For knitting document and include_graphics function
library(png)
# For grabbing the dimensions of png files
include_graphics("emotion_plot.png")
library(knitr)    # For knitting document and include_graphics function
library(png)
# For grabbing the dimensions of png files
include_graphics("sentiment_plot.png")
# for knitting the document and enabling the include_graphics function
library(knitr)
library(png)
# for determining the dimensions of PNG files
include_graphics("wordcloud.png")
library(knitr)    # For knitting document and include_graphics function
library(png)
# For grabbing the dimensions of png files
include_graphics("topic.png")
#loading the library
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)
#read in the file
file<-read.csv("witcher3_tw_cn.csv")
tweets.df<-file$x
# tweets.df<-tolower(tweets.df)
# tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))
#
# #get rid of unnecessary spaces
# tweets.df <- str_replace_all(tweets.df," "," ")
# # Get rid of URLs
# #tweets.df <- str_replace_all(tweets.df, "http://t.co/[a-z,A-Z,0-9]*{8}","")
# # Take out retweet header, there is only one
# tweets.df <- str_replace(tweets.df,"RT @[a-z,A-Z]*: ","")
# # Get rid of hashtags
# tweets.df <- str_replace_all(tweets.df,"#[a-z,A-Z]*","")
# # Get rid of references to other screennames
# tweets.df <- str_replace_all(tweets.df,"@[a-z,A-Z]*","")
#view cleaned tweets
# View(tweets.df)
#Reading the Lexicon positive and negative words
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")
#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
# Parameters
# sentences: vector of text to score
# pos.words: vector of words of postive sentiment
# neg.words: vector of words of negative sentiment
# .progress: passed to laply() to control of progress bar
# create simple array of scores with laply
scores <- laply(sentences,
function(sentence, pos.words, neg.words)
{
# remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
# remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
# remove digits
sentence <- gsub('\\d+', '', sentence)
#convert to lower
sentence <- tolower(sentence)
# split sentence into words with str_split (stringr package)
word.list <- str_split(sentence, "\\s+")
words <- unlist(word.list)
# compare words to the dictionaries of positive & negative terms
pos.matches <- match(words, pos)
neg.matches <- match(words, neg)
# get the position of the matched term or NA
# we just want a TRUE/FALSE
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
# final score
score <- sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
# data frame with scores for each sentence
scores.df <- data.frame(text=sentences, score=scores)
return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(tweets.df, pos.txt, neg.txt, .progress='text')
# View(scores_twitter)
#Summary of the sentiment scores
summary(scores_twitter)
scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negtive', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))
# View(scores_twitter)
#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")
#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) +
scale_y_continuous(labels = percent)+labs(y="Score")+
theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1
#install.packages("LDAvis")
#install.packages("tm")
#install.packages("lda")
#install.packages("servr")
#install.packages("shiny")
#install.packages("stringr")
library(LDAvis)
library(tm)
library(lda)
library(shiny)
library(stringr)
stop_words <- stopwords("SMART")
Dataset2<-read.csv("witcher3_tw_cn.csv")
tweet <- Dataset2$x
tweet <- sapply(tweet, function(x) iconv(x, to='UTF-8', sub='byte'))
tweet= gsub("[[:punct:]]", "", tweet)
tweet = gsub("[[:digit:]]", "", tweet)
tweet= gsub("http\\w+", "", tweet)
tweet = gsub("[ \t]{2,}", "", tweet)
tweet= gsub("^\\s+|\\s+$", "", tweet)
#ref: ( Hicks , 2014)
#get rid of unnecessary spaces
tweet <- str_replace_all(tweet," "," ")
tweet <- str_replace(tweet,"RT @[a-z,A-Z]*: ","")
# Get rid of hashtags
tweet <- str_replace_all(tweet,"#[a-z,A-Z]*","")
# Get rid of references to other screennames
tweet<- str_replace_all(tweet,"@[a-z,A-Z]*","")
# tokenize on space and output as a list:
doc.list <- strsplit(tweet, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
# now put the documents into the format required by the lda package:
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents
W <- length(vocab)  # number of terms in the vocab
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document
N <- sum(doc.length)  # total number of tokens in the data
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus
# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = 10, vocab = vocab,
num.iterations = 200, alpha = 0.5, eta=0.5,
initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1
#LDAvis
theta <- t(apply(fit$document_sums + 0.5, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + 0.5, 2, function(x) x/sum(x)))
tweetvis <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
# create the JSON object to feed the visualization:
json <- createJSON(phi = tweetvis$phi,
theta = tweetvis$theta,
doc.length = tweetvis$doc.length,
vocab = tweetvis$vocab,
term.frequency = tweetvis$term.frequency)
serVis(json, out.dir = tempfile(), open.browser = interactive())
