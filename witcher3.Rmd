---
title: "The text mining of game reviews for 'The Witcher 3' "
# author: "Jiakun Peng"
# date: "2020/3/24"
output: html_document
---
## Text Mining on Twitter reviews
- Extracting tweets from twitter api
- Data Cleaninig
- Emotion Detection (Category analysis and polarity analysis of emotions)
- Keywords extraction and analysis (WordCloud and Topic Model)
- Maping twitter users (key word)

## 1. Extracting tweets with specific hashtags or keywords

First, we try to extract reviews from the social media website Twitter. In order to obtain the reviews, We use the four authentication keys from Twitter and set specific keywords to extract the comments. In the code snippet below, we have extracted 3000 tweets with the keyword "The Witcher 3" and set the comment language to "English". These parameters can be adjusted as necessary.And at last, save the extracted comments to a CSV file.

```{r, eval=FALSE}


library(twitteR)
library(ROAuth)

# authenticate with Twitter

consumerKey<-	"G51KUQTqdYzMWk0XBoHpGPah2"
consumerSecret<-"9ILQpHv5E5QElY8bfRzqy9Z3b8WIeOctBC4vlX2CplPnPaUgUF"

accessToken<-"1844121-FnMyAIkRngSjqEsniQsTOQvyIYkGQWvKHGXJLlkMHJ"
accessSecret<-"A9hLM7MSfTmIxlN5fRKfQZmeezn0FamNXKWqyzeeJVJAN"

setup_twitter_oauth (consumerKey, consumerSecret, accessToken, accessSecret)  # authenticate

# search for tweets by keyword

tweets<-searchTwitter("The Witcher 3", n=3000, lang="en", since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL,
                                resultType=NULL, retryOnRateLimit=120)

# put tweets in a data frame
tweets<-twListToDF(tweets)

# write out to a CSV file
write.csv(tweets, file="witcher3_tw.csv")

```

## 2. Data Cleaning

The next step is to clean the reviews. Since the comments on twitter were very casual, with lots of mistakes and abbreviations in grammar and words.Meanwhile, there are a lot of advertisements, emoticons, special symbols and so on in the comments on twitter.We have to clear this and keep only the text that contains the correct content.The specific cleaning process is as follows:

* Load the original text
* Convert text to lowercase
* Get rid of problem characters
* remove punctuation, digits, special characters etc
* get rid of unnecessary spaces
* get rid of URLs
* take out the retweet header (there is only one)
* get rid of hashtags
* get rid of references to other screen names


```{r, eval=FALSE}
# install these packages if you do not have them installed yet
#install.packages("twitteR")
#install.packages("plyr")
#install.packages("stringr")
#install.packages("tm", repos="http://R-Forge.R-project.org")
#install.packages("NLP")
#install.packages("xml2")
#install.packages("ggplot2")
# load the libraries
library(twitteR)
library(plyr)
library(stringr)
library(ggplot2)
library(tm)

# import your data set to analyse,
# ensure it is in the same directory as your code, otherwise you need to add the path
Dataset2 <- read.csv("witcher3_tw.csv")
library(stringi)
Dataset2$text<-stringi::stri_trans_general(Dataset2$text, "latin-ascii")
tweets.df <- Dataset2$text

# convert text to lowercase
tweets.df<-tolower(tweets.df)

# get rid of problem characters
tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))

# remove punctuation, digits, special characters etc
tweets.df = gsub("&amp", "", tweets.df)
tweets.df= gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets.df)
tweets.df = gsub("@\\w+", "", tweets.df)
tweets.df= gsub("[[:punct:]]", "", tweets.df)
tweets.df = gsub("[[:digit:]]", "", tweets.df)
tweets.df = gsub("http\\w+", "", tweets.df)
tweets.df = gsub("[ \t]{2,}", "", tweets.df)
tweets.df= gsub("^\\s+|\\s+$", "", tweets.df) 


# get rid of unnecessary spaces
tweets.df <- str_replace_all(tweets.df," "," ")

# get rid of URLs
tweets.df <- str_replace_all(tweets.df, "http://t.co/[a-z,A-Z,0-9]*{8}","")

# take out the retweet header (there is only one)
tweets.df <- str_replace(tweets.df,"RT @[a-z,A-Z]*: ","")

# get rid of hashtags
tweets.df <- str_replace_all(tweets.df,"#[a-z,A-Z]*","")

# get rid of references to other screen names
tweets.df <- str_replace_all(tweets.df,"@[a-z,A-Z]*","")  

#View(tweets.df)


# write out to a CSV file
write.csv(tweets.df, file="witcher3_tw_cn.csv",row.names = F)

```

## 3.1. Detecting Sentiment Categories
This step is to detect different sentiment categories form the comments. The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). In this step, we only detect the eight emotions based on the processed comments through using the NRC emotion lexicon.

From the result image, the main type of emotion in the comments on twitter was anticipation, followed by trust, fear and joy. Surprise, anger and disgust were the least. So, we can consider that reviewers were filled with acticipation for the game 'The Witcher 3'. And when they were playing the game, they felt happy and terrified.

```{r, eval= FALSE, message=FALSE}
library(syuzhet)
library(plotly)
library(tm)
library(wordcloud)

#import your dataset to analyse, 
#ensure it is in the same directory as your code, 
#otherwise you need to add the path
  
  tweets <- read.csv("witcher3_tw_cn.csv")
  clean_tweets = tweets$x
  clean_tweets = tolower(clean_tweets)
  
  emotions <- get_nrc_sentiment(clean_tweets)
  emo_bar = colSums(emotions)
  emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
  emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])
  
  emo_sum <- emo_sum[1:8,]
  emo_sum$percent<-(emo_sum$count/sum(emo_sum$count))*100
  
   #Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~percent, type="bar", color=~emotion) %>%
layout(xaxis=list(title=""),  yaxis = list(title = "Emotion count"),
showlegend=FALSE,title="Distribution of emotion categories") %>%
layout(yaxis = list(ticksuffix = "%"))
``` 
  

```{r echo=FALSE,out.width="50%",fig.align= "center", fig.cap= " Emotion categories for a text corpus", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("tw_emotion_plot.png")
```

## 3.2.Detecting Sentiment Polarity
In this step, we try to detect the sentiment polarity from the processed comments. This code uses an external sentiment lexicon to detect the polarity of a text corpora. It classifies the sentiment of a piece of text as either positive, negative or neutral using the positive and negative sentiment lexicons. 

From the resulting images, we can see that the comments on twitter about 'The Witcher 3' are neutral to the majority, followed by positive, and finally negative.Since the comments on twitter are so casual, a large number of words are not included in the negative or positive vocabulary list, so these words are assigned to neutral emotions, resulting in neutral emotions being much higher than negative and positive emotions.The fact that positive emotions outnumber negative emotions indicates that the game performed well and was liked by most players.


```{r, eval= FALSE, message=FALSE}

#loading the library
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)


#read in the file
file<-read.csv("witcher3_tw_cn.csv")
tweets.df<-file$x
# tweets.df<-tolower(tweets.df)
# tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))
# 
# #get rid of unnecessary spaces
# tweets.df <- str_replace_all(tweets.df," "," ")
# # Get rid of URLs
# #tweets.df <- str_replace_all(tweets.df, "http://t.co/[a-z,A-Z,0-9]*{8}","")
# # Take out retweet header, there is only one
# tweets.df <- str_replace(tweets.df,"RT @[a-z,A-Z]*: ","")
# # Get rid of hashtags
# tweets.df <- str_replace_all(tweets.df,"#[a-z,A-Z]*","")
# # Get rid of references to other screennames
# tweets.df <- str_replace_all(tweets.df,"@[a-z,A-Z]*","")  

#view cleaned tweets
# View(tweets.df)


#Reading the Lexicon positive and negative words
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")

#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
  # Parameters
  # sentences: vector of text to score
  # pos.words: vector of words of postive sentiment
  # neg.words: vector of words of negative sentiment
  # .progress: passed to laply() to control of progress bar
  
  # create simple array of scores with laply
  scores <- laply(sentences,
                  function(sentence, pos.words, neg.words)
                  {
                    # remove punctuation
                    sentence <- gsub("[[:punct:]]", "", sentence)
                    # remove control characters
                    sentence <- gsub("[[:cntrl:]]", "", sentence)
                    # remove digits
                    sentence <- gsub('\\d+', '', sentence)
                    
                    #convert to lower
                    sentence <- tolower(sentence)
                    
                    
                    # split sentence into words with str_split (stringr package)
                    word.list <- str_split(sentence, "\\s+")
                    words <- unlist(word.list)
                    
                    # compare words to the dictionaries of positive & negative terms
                    pos.matches <- match(words, pos)
                    neg.matches <- match(words, neg)
                    
                    # get the position of the matched term or NA
                    # we just want a TRUE/FALSE
                    pos.matches <- !is.na(pos.matches)
                    neg.matches <- !is.na(neg.matches)
                    
                    # final score
                    score <- sum(pos.matches) - sum(neg.matches)
                    return(score)
                  }, pos.words, neg.words, .progress=.progress )
  # data frame with scores for each sentence
  scores.df <- data.frame(text=sentences, score=scores)
  return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(tweets.df, pos.txt, neg.txt, .progress='text')


# View(scores_twitter)

#Summary of the sentiment scores
summary(scores_twitter)

scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negtive', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))


# View(scores_twitter)


#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")  

#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_y_continuous(labels = percent)+labs(y="Score")+
  theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1

```


```{r echo=FALSE,out.width="50%",fig.align="center", fig.cap= "Sentiment Polarity of a text corpus", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("tw_sentiment_plot.png")
```
## 4.1  Generating a word cloud

At this stage we wanted to get the key words in the reviews to see what the reviewers had to say about the game.We visualized the extracted keywords through word clouds, so that we could more intuitively observe which keywords occupy the mainstream.

The result shows the keyword 'Witcher', 'witcherwild', 'games', 'hunt', 'recommend' are the keyword that appears the most times. In fact, the comments on twitter are so casual that most of the keywords don't make much sense, but we can see from the keywords' witcherwild 'and' hunt 'that this is a game about hunting, and 'recommend' indicates that most reviewers will recommend this game to others.It also shows that the game is highly rated.

```{r, eval=FALSE}

# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)

#library(wordcloud2)
dataset <- read.csv("witcher3_tw_cn.csv")
# get the text column
tweets.df<- dataset$x

# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df)) 
tweet_corpus
inspect(tweet_corpus[1])

# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(4,0.6))
```


```{r echo=FALSE, out.width="100%",fig.align= "center", fig.cap="Word Cloud of a text corpus", message=FALSE, warning=FALSE}

# for knitting the document and enabling the include_graphics function
library(knitr)    
library(png)

# for determining the dimensions of PNG files
include_graphics("wordcloud_tw.png")
```

## 4.2 Generating a topic model

At this stage, we try to obtain the topic model of the processed comments, so as to dig deeper into the topic of the user's comments, and try to find something interesting.

To get the topic model, we use LDAvis, a web-based interactive visualization of topics estimated using Latent Dirichlet Allocation. And in the code below, we captured 10 topics. 

From the result, we can find that the most frequently used word in topic 8 is 'netfix' which is completely different from the other topics. Other words that appear in topic 8 are 'watch', 'shows'. The main reason for this is that 'The Witcher' have recently been made into a TV series that can be viewed on netflix, and there's been a fair amount of discussion about it, so we can see these keywords in the comments of the game 'The Withcer 3'. In addition, we can also see the word cyberpunk in topic 8.This is a new project being developed by 'The Witcher 3' developer CD PROJEKT RED, which has also generated a lot of discussion.


```{r, eval= FALSE, message=FALSE}
#install.packages("LDAvis")
#install.packages("tm")
#install.packages("lda")
#install.packages("servr")
#install.packages("shiny")
#install.packages("stringr")

library(LDAvis)
library(tm)
library(lda)
library(shiny)
library(stringr)

stop_words <- stopwords("SMART")

Dataset2<-read.csv("witcher3_tw_cn.csv")
tweet <- Dataset2$x


tweet <- sapply(tweet, function(x) iconv(x, to='UTF-8', sub='byte'))


tweet= gsub("[[:punct:]]", "", tweet)
tweet = gsub("[[:digit:]]", "", tweet)
tweet= gsub("http\\w+", "", tweet)
tweet = gsub("[ \t]{2,}", "", tweet)
tweet= gsub("^\\s+|\\s+$", "", tweet) 
#ref: ( Hicks , 2014) 

#get rid of unnecessary spaces
tweet <- str_replace_all(tweet," "," ")

tweet <- str_replace(tweet,"RT @[a-z,A-Z]*: ","")
# Get rid of hashtags
tweet <- str_replace_all(tweet,"#[a-z,A-Z]*","")
# Get rid of references to other screennames
tweet<- str_replace_all(tweet,"@[a-z,A-Z]*","")  

# tokenize on space and output as a list:
doc.list <- strsplit(tweet, "[[:space:]]+")

# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)

# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)

# Compute some statistics related to the data set:
D <- length(documents)  # number of documents 
W <- length(vocab)  # number of terms in the vocab 
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document 
N <- sum(doc.length)  # total number of tokens in the data 
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus 


# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02

# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = 10, vocab = vocab, 
                                   num.iterations = 200, alpha = 0.5, eta=0.5,
                                    initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  

#LDAvis
theta <- t(apply(fit$document_sums + 0.5, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + 0.5, 2, function(x) x/sum(x)))


tweetvis <- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)


# create the JSON object to feed the visualization:
json <- createJSON(phi = tweetvis$phi, 
                   theta = tweetvis$theta, 
                   doc.length = tweetvis$doc.length, 
                   vocab = tweetvis$vocab, 
                   term.frequency = tweetvis$term.frequency)
serVis(json, out.dir = tempfile(), open.browser = interactive())
```

The result is an interactive webpage where you can view the different topics and select the number of of terms to include in a topic. You can read up more info on LDA visualization in this paper (sievert, 2014) https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf

```{r echo=FALSE, out.width="100%",out.height="100%", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("topic_tw.png")
```

## 5. Maping twitter users (key word)
In this stage, we mainly get the region where the reviewer is located, and we use the API of Google map to achieve this.The specific process is as follows:

- Go to the Google Cloud Platform Console.
- From the Project drop-down menu, select or create the project for which you want to add an API key.
- From the  Navigation menu, select APIs & Services > Credentials.
On the Credentials page, click Create credentials > API key. 
The API key created dialog displays your newly created API key. Copy this key as insert it in the relevant place in your R code
- On the dialog, click Restrict Key. 
(For more information, see Restricting an API key.)
On the API key page, under Key restrictions, set the Application restrictions.

Since the comments we use are in English, It is to be expected that the reviewer's region will be concentrated in countries where the official language is English. All we have to do is try to find the patterns of the countries where the reviewers are concentrated. From the result, We can find that the most concentrated areas of commenters are the United States and European countries. As the developer of ‘The Witcher 3’ is Poland, so the European countries are influenced by it, there are many reviewers. And the United States also has a large number of players and game developers, the game atmosphere is very rich, so there is also a large number of reviewers. 

```{r, eval= FALSE, message=FALSE}
#######
#Step 1: Enable pings to the Twitter API:
#######
#Load / install "twitteR" package:
if (!require("twitteR")) {
  install.packages("twitteR", repos="http://cran.rstudio.com/")
  library("twitteR")
}
consumerKey <-"G51KUQTqdYzMWk0XBoHpGPah2"
consumerSecret<-"9ILQpHv5E5QElY8bfRzqy9Z3b8WIeOctBC4vlX2CplPnPaUgUF"
accessToken<-"1844121-FnMyAIkRngSjqEsniQsTOQvyIYkGQWvKHGXJLlkMHJ"
accessSecret<-"A9hLM7MSfTmIxlN5fRKfQZmeezn0FamNXKWqyzeeJVJAN"
#Log your info with the Twitter API:
options(httr_oauth_cache=T) #This will enable the use of a local file to cache OAuth access credentials between R sessions.
setup_twitter_oauth(consumerKey,
                    consumerSecret,
                    accessToken,
                    accessSecret)
#######
# Step 2: Download the Followers of a Given Twitter Account
library(rtweet)
search<-search_tweets("The Witcher 3", n=3000)
#######


if (!require("data.table")) {
  install.packages("data.table", repos="http://cran.rstudio.com/")
  library("data.table")
}
#Turn this data into a data frame:
search<-subset(search, location!="")
#######
# Step 4: Geocode Followers' Locations
#######
#Remove special characters:
search$location<-gsub("%", " ",search$location)
#Install key package helpers:
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/geocode_helpers.R")
#Install modified version of the geocode function
#(that now includes the api_key parameter):
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/modified_geocode.R")
#Generate specific geocode function:
write.csv(search$location,'location.csv')
geocode_apply<-function(x){
  geocode(x, source = "google", output = "all", api_key="AIzaSyDtUGziXwtqjWz_P0hHN4kIP7z9lHinUaU")
}
#Apply this new function to entire list:
geocode_results<-sapply(search$location, geocode_apply, simplify = F)
#Look at the number of geocoded locations:
length(geocode_results)
#######
#Step 5: Clean Geocoding Results
#######

#Only keep locations with "status" = "ok"
condition_a <- sapply(geocode_results, function(x) x["status"]=="OK")
geocode_results<-geocode_results[condition_a]
#Only keep locations with one match:
condition_b <- lapply(geocode_results, lapply, length)
condition_b2<-sapply(condition_b, function(x) x["results"]=="1")
geocode_results<-geocode_results[condition_b2]
#Look at the number of *successfully* geocoded locations:
length(geocode_results)
#Address formatting issues:
source("https://raw.githubusercontent.com/LucasPuente/geocoding/master/cleaning_geocoded_results.R")
#Turn list into a data.frame:
results_b<-lapply(geocode_results, as.data.frame)
results_c<-lapply(results_b,function(x) subset(x, select=c("results.formatted_address", "results.geometry.location")))
#Format thes new data frames:
results_d<-lapply(results_c,function(x) data.frame(Location=x[1,"results.formatted_address"],
                                                   lat=x[1,"results.geometry.location"],
                                                   lng=x[2,"results.geometry.location"]))
#Bind these data frames together:
results_e<-rbindlist(results_d)
#Add info on the original (i.e. user-provided) locatio/'-n string:
results_f<-results_e[,Original_Location:=names(results_d)]
#Only keep American results:
#install.packages("leaflet")
library(leaflet)
site_locations <- leaflet(results_f) %>%
  addTiles() %>%
  addCircleMarkers(lng = ~lng, lat = ~lat, popup = ~results_f,
                   radius = 3, stroke = FALSE)

site_locations

#install.packages("mapview")
library(mapview)
mapshot(site_locations, url = paste0(getwd(), "/map.html"),
        file = paste0(getwd(), "/map.png"))


##
```


```{r echo=FALSE, fig.align="center",fig.cap="Geo-map of 2000 tweets about Witcher 3",out.width="100%", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files

include_graphics("map_tw.png")
#\captionof{figure}{caption}

```

---
## Text Mining on Steam reviews
- Extracting tweets from Python
- Data Cleaninig
- Emotion Detection (Category analysis and polarity analysis of emotions)
- Keywords extraction and analysis (WordCloud and Topic Model)

## 1. Extracting Steam Reviews on Witcher 3
Since Steam doesn't provide an R language API to crawl comments, we wrote a crawler in Python to crawl comments.

## 2. Data Cleaning




```{r, eval=FALSE}
# install these packages if you do not have them installed yet
#install.packages("twitteR")
#install.packages("plyr")
#install.packages("stringr")
#install.packages("tm", repos="http://R-Forge.R-project.org")
#install.packages("NLP")
#install.packages("xml2")
#install.packages("ggplot2")
# load the libraries
library(twitteR)
library(plyr)
library(stringr)
library(ggplot2)
library(tm)

# import your data set to analyse,
# ensure it is in the same directory as your code, otherwise you need to add the path
Dataset2 <- read.csv("witcher3.csv")
library(stringi)
Dataset2$text<-stringi::stri_trans_general(Dataset2$text, "latin-ascii")
tweets.df <- Dataset2$text

# convert text to lowercase
tweets.df<-tolower(tweets.df)

# get rid of problem characters
tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))

# remove punctuation, digits, special characters etc
tweets.df = gsub("&amp", "", tweets.df)
tweets.df= gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tweets.df)
tweets.df = gsub("@\\w+", "", tweets.df)
tweets.df= gsub("[[:punct:]]", "", tweets.df)
tweets.df = gsub("[[:digit:]]", "", tweets.df)
tweets.df = gsub("http\\w+", "", tweets.df)
tweets.df = gsub("[ \t]{2,}", "", tweets.df)
tweets.df= gsub("^\\s+|\\s+$", "", tweets.df) 


# get rid of unnecessary spaces
tweets.df <- str_replace_all(tweets.df," "," ")

# get rid of URLs
#tweets.df <- str_replace_all(tweets.df, "http://t.co/[a-z,A-Z,0-9]*{8}","")

# take out the retweet header (there is only one)
tweets.df <- str_replace(tweets.df,"RT @[a-z,A-Z]*: ","")

# get rid of hashtags
tweets.df <- str_replace_all(tweets.df,"#[a-z,A-Z]*","")

# get rid of references to other screen names
tweets.df <- str_replace_all(tweets.df,"@[a-z,A-Z]*","")  

#View(tweets.df)


# write out to a CSV file
write.csv(tweets.df, file="witcher3_cn.csv",row.names = F)

```

## 3.1. Detecting Sentiment Categories


```{r, eval= FALSE, message=FALSE}
library(syuzhet)
library(plotly)
library(tm)
library(wordcloud)

#import your dataset to analyse, 
#ensure it is in the same directory as your code, 
#otherwise you need to add the path
  
  tweets <- read.csv("witcher3_cn.csv")
  clean_tweets = tweets$x
  clean_tweets = tolower(clean_tweets)
  
  emotions <- get_nrc_sentiment(clean_tweets)
  emo_bar = colSums(emotions)
  emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
  emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])
  
  emo_sum <- emo_sum[1:8,]
  emo_sum$percent<-(emo_sum$count/sum(emo_sum$count))*100
  
   #Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~percent, type="bar", color=~emotion) %>%
layout(xaxis=list(title=""),  yaxis = list(title = "Emotion count"),
showlegend=FALSE,title="Distribution of emotion categories") %>%
layout(yaxis = list(ticksuffix = "%"))
``` 
  

```{r echo=FALSE,out.width="50%",fig.align= "center", fig.cap= " Emotion categories for a text corpus", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("emotion_plot.png")
```

## 3.2.Detecting Sentiment Polarity



```{r, eval= FALSE, message=FALSE}

#loading the library
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)


#read in the file
file<-read.csv("witcher3_cn.csv")
tweets.df<-file$x
tweets.df<-tolower(tweets.df)
tweets.df <- sapply(tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))

#get rid of unnecessary spaces
tweets.df <- str_replace_all(tweets.df," "," ")
# Get rid of URLs
#tweets.df <- str_replace_all(tweets.df, "http://t.co/[a-z,A-Z,0-9]*{8}","")
# Take out retweet header, there is only one
tweets.df <- str_replace(tweets.df,"RT @[a-z,A-Z]*: ","")
# Get rid of hashtags
tweets.df <- str_replace_all(tweets.df,"#[a-z,A-Z]*","")
# Get rid of references to other screennames
tweets.df <- str_replace_all(tweets.df,"@[a-z,A-Z]*","")  

#view cleaned tweets
# View(tweets.df)


#Reading the Lexicon positive and negative words
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")

#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
  # Parameters
  # sentences: vector of text to score
  # pos.words: vector of words of postive sentiment
  # neg.words: vector of words of negative sentiment
  # .progress: passed to laply() to control of progress bar
  
  # create simple array of scores with laply
  scores <- laply(sentences,
                  function(sentence, pos.words, neg.words)
                  {
                    # remove punctuation
                    sentence <- gsub("[[:punct:]]", "", sentence)
                    # remove control characters
                    sentence <- gsub("[[:cntrl:]]", "", sentence)
                    # remove digits
                    sentence <- gsub('\\d+', '', sentence)
                    
                    #convert to lower
                    sentence <- tolower(sentence)
                    
                    
                    # split sentence into words with str_split (stringr package)
                    word.list <- str_split(sentence, "\\s+")
                    words <- unlist(word.list)
                    
                    # compare words to the dictionaries of positive & negative terms
                    pos.matches <- match(words, pos)
                    neg.matches <- match(words, neg)
                    
                    # get the position of the matched term or NA
                    # we just want a TRUE/FALSE
                    pos.matches <- !is.na(pos.matches)
                    neg.matches <- !is.na(neg.matches)
                    
                    # final score
                    score <- sum(pos.matches) - sum(neg.matches)
                    return(score)
                  }, pos.words, neg.words, .progress=.progress )
  # data frame with scores for each sentence
  scores.df <- data.frame(text=sentences, score=scores)
  return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(tweets.df, pos.txt, neg.txt, .progress='text')


# View(scores_twitter)

#Summary of the sentiment scores
summary(scores_twitter)

scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negtive', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))


# View(scores_twitter)


#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")  

#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_y_continuous(labels = percent)+labs(y="Score")+
  theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1

```


```{r echo=FALSE,out.width="50%",fig.align="center", fig.cap= "Sentiment Polarity of a text corpus", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("sentiment_plot.png")
```
## 4.1  Generating a word cloud



```{r, eval=FALSE}

# load the libraries
#install.packages("wordcloud")
library(plyr)
library(stringr)
library(tm)
library(wordcloud)

#library(wordcloud2)
dataset <- read.csv("witcher3_cn.csv")
# get the text column
tweets.df<- dataset$x

# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(tweets.df)) 
tweet_corpus
inspect(tweet_corpus[1])

# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(4,0.6))
```


```{r echo=FALSE, out.width="100%",fig.align= "center", fig.cap="Word Cloud of a text corpus", message=FALSE, warning=FALSE}

# for knitting the document and enabling the include_graphics function
library(knitr)    
library(png)

# for determining the dimensions of PNG files
include_graphics("wordcloud.png")
```

## 4.2 Generating a topic model




```{r, eval= FALSE, message=FALSE}
#install.packages("LDAvis")
#install.packages("tm")
#install.packages("lda")
#install.packages("servr")
#install.packages("shiny")
#install.packages("stringr")

library(LDAvis)
library(tm)
library(lda)
library(shiny)
library(stringr)

stop_words <- stopwords("SMART")

Dataset2<-read.csv("witcher3_cn.csv")
tweet <- Dataset2$x


tweet <- sapply(tweet, function(x) iconv(x, to='UTF-8', sub='byte'))


tweet= gsub("[[:punct:]]", "", tweet)
tweet = gsub("[[:digit:]]", "", tweet)
tweet= gsub("http\\w+", "", tweet)
tweet = gsub("[ \t]{2,}", "", tweet)
tweet= gsub("^\\s+|\\s+$", "", tweet) 
#ref: ( Hicks , 2014) 

#get rid of unnecessary spaces
tweet <- str_replace_all(tweet," "," ")

tweet <- str_replace(tweet,"RT @[a-z,A-Z]*: ","")
# Get rid of hashtags
tweet <- str_replace_all(tweet,"#[a-z,A-Z]*","")
# Get rid of references to other screennames
tweet<- str_replace_all(tweet,"@[a-z,A-Z]*","")  

# tokenize on space and output as a list:
doc.list <- strsplit(tweet, "[[:space:]]+")

# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)

# remove terms that are stop words or occur fewer than 5 times:
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)

# now put the documents into the format required by the lda package:
get.terms <- function(x) {
  index <- match(x, vocab)
  index <- index[!is.na(index)]
  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)

# Compute some statistics related to the data set:
D <- length(documents)  # number of documents 
W <- length(vocab)  # number of terms in the vocab 
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document 
N <- sum(doc.length)  # total number of tokens in the data 
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus 


# MCMC and model tuning parameters:
K <- 20
G <- 5000
alpha <- 0.02
eta <- 0.02

# Fit the model:
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = 10, vocab = vocab, 
                                   num.iterations = 200, alpha = 0.5, eta=0.5,
                                    initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1  

#LDAvis
theta <- t(apply(fit$document_sums + 0.5, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + 0.5, 2, function(x) x/sum(x)))


tweetvis <- list(phi = phi,
                     theta = theta,
                     doc.length = doc.length,
                     vocab = vocab,
                     term.frequency = term.frequency)


# create the JSON object to feed the visualization:
json <- createJSON(phi = tweetvis$phi, 
                   theta = tweetvis$theta, 
                   doc.length = tweetvis$doc.length, 
                   vocab = tweetvis$vocab, 
                   term.frequency = tweetvis$term.frequency)
serVis(json, out.dir = tempfile(), open.browser = interactive())
```


```{r echo=FALSE, out.width="100%",out.height="100%", message=FALSE, warning=FALSE}
library(knitr)    # For knitting document and include_graphics function
library(png)   
# For grabbing the dimensions of png files
include_graphics("topic.png")
```