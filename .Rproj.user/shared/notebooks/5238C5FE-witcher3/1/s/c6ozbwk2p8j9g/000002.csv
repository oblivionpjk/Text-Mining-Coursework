"0","#install.packages(""LDAvis"")"
"0","#install.packages(""tm"")"
"0","#install.packages(""lda"")"
"0","#install.packages(""servr"")"
"0","#install.packages(""shiny"")"
"0","#install.packages(""stringr"")"
"0",""
"0","library(LDAvis)"
"2","程辑包‘LDAvis’是用R版本3.6.3 来建造的"
"0","library(tm)"
"0","library(lda)"
"2","程辑包‘lda’是用R版本3.6.3 来建造的"
"0","library(shiny)"
"2","程辑包‘shiny’是用R版本3.6.3 来建造的"
"0","library(stringr)"
"0",""
"0","stop_words <- stopwords(""SMART"")"
"0",""
"0","Dataset2<-read.csv(""witcher3_tw_cn.csv"")"
"0","tweet <- Dataset2$x"
"0",""
"0",""
"0","tweet <- sapply(tweet, function(x) iconv(x, to='UTF-8', sub='byte'))"
"0",""
"0",""
"0","tweet= gsub(""[[:punct:]]"", """", tweet)"
"0","tweet = gsub(""[[:digit:]]"", """", tweet)"
"0","tweet= gsub(""http\\w+"", """", tweet)"
"0","tweet = gsub(""[ \t]{2,}"", """", tweet)"
"0","tweet= gsub(""^\\s+|\\s+$"", """", tweet) "
"0","#ref: ( Hicks , 2014) "
"0",""
"0","#get rid of unnecessary spaces"
"0","tweet <- str_replace_all(tweet,"" "","" "")"
"0",""
"0","tweet <- str_replace(tweet,""RT @[a-z,A-Z]*: "","""")"
"0","# Get rid of hashtags"
"0","tweet <- str_replace_all(tweet,""#[a-z,A-Z]*"","""")"
"0","# Get rid of references to other screennames"
"0","tweet<- str_replace_all(tweet,""@[a-z,A-Z]*"","""")  "
"0",""
"0","# tokenize on space and output as a list:"
"0","doc.list <- strsplit(tweet, ""[[:space:]]+"")"
"0",""
"0","# compute the table of terms:"
"0","term.table <- table(unlist(doc.list))"
"0","term.table <- sort(term.table, decreasing = TRUE)"
"0",""
"0","# remove terms that are stop words or occur fewer than 5 times:"
"0","del <- names(term.table) %in% stop_words | term.table < 5"
"0","term.table <- term.table[!del]"
"0","vocab <- names(term.table)"
"0",""
"0","# now put the documents into the format required by the lda package:"
"0","get.terms <- function(x) {"
"0","  index <- match(x, vocab)"
"0","  index <- index[!is.na(index)]"
"0","  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))"
"0","}"
"0","documents <- lapply(doc.list, get.terms)"
"0",""
"0","# Compute some statistics related to the data set:"
"0","D <- length(documents)  # number of documents "
"0","W <- length(vocab)  # number of terms in the vocab "
"0","doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document "
"0","N <- sum(doc.length)  # total number of tokens in the data "
"0","term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus "
"0",""
"0",""
"0","# MCMC and model tuning parameters:"
"0","K <- 20"
"0","G <- 5000"
"0","alpha <- 0.02"
"0","eta <- 0.02"
"0",""
"0","# Fit the model:"
"0","library(lda)"
"0","set.seed(357)"
"0","t1 <- Sys.time()"
"0","fit <- lda.collapsed.gibbs.sampler(documents = documents, K = 10, vocab = vocab, "
"0","                                   num.iterations = 200, alpha = 0.5, eta=0.5,"
"0","                                    initial = NULL, burnin = 0,"
"0","                                   compute.log.likelihood = TRUE)"
"0","t2 <- Sys.time()"
"0","t2 - t1  "
"1","Time difference of "
"1",""
"1","1.493985"
"1",""
"1"," "
"1",""
"1","secs"
"1",""
"1","
"
"0","#LDAvis"
"0","theta <- t(apply(fit$document_sums + 0.5, 2, function(x) x/sum(x)))"
"0","phi <- t(apply(t(fit$topics) + 0.5, 2, function(x) x/sum(x)))"
"0",""
"0",""
"0","tweetvis <- list(phi = phi,"
"0","                     theta = theta,"
"0","                     doc.length = doc.length,"
"0","                     vocab = vocab,"
"0","                     term.frequency = term.frequency)"
"0",""
"0",""
"0","# create the JSON object to feed the visualization:"
"0","json <- createJSON(phi = tweetvis$phi, "
"0","                   theta = tweetvis$theta, "
"0","                   doc.length = tweetvis$doc.length, "
"0","                   vocab = tweetvis$vocab, "
"0","                   term.frequency = tweetvis$term.frequency)"
"0","serVis(json, out.dir = tempfile(), open.browser = interactive())"
